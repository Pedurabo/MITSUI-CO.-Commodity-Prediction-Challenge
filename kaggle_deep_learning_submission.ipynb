{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\ude80 MITSUI&CO. Commodity Prediction Challenge - Deep Learning Submission\n",
        "\n",
        "This notebook implements a comprehensive deep learning solution using LSTM, GRU, and Transformer models with advanced feature engineering and ensemble methods.\n",
        "\n",
        "## \ud83c\udfaf Key Features\n",
        "- **Deep Learning Models**: LSTM, GRU, Transformer, Hybrid CNN-LSTM\n",
        "- **Advanced Feature Engineering**: Lag features, rolling statistics, cross-asset relationships\n",
        "- **Multi-Target Learning**: All 424 targets with shared representations\n",
        "- **Ensemble Methods**: Weighted averaging and stacking\n",
        "- **Hyperparameter Optimization**: Optuna-based tuning\n",
        "\n",
        "## \ud83d\udcca Competition Details\n",
        "- **Evaluation Metric**: Sharpe Ratio variant\n",
        "- **Targets**: 424 prediction targets\n",
        "- **Data**: Multi-market commodity data\n",
        "- **Runtime Limit**: 8 hours\n",
        "- **Memory Limit**: 16 GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "!pip install optuna scikit-learn pandas numpy matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Any\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.pruners import MedianPruner\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data loading and preprocessing functions\n",
        "def load_data():\n",
        "    \"\"\"Load competition data\"\"\"\n",
        "    print(\"Loading data...\")\n",
        "    \n",
        "    train_data = pd.read_csv('/kaggle/input/mitsui-commodity-prediction-challenge/train.csv')\n",
        "    test_data = pd.read_csv('/kaggle/input/mitsui-commodity-prediction-challenge/test.csv')\n",
        "    train_labels = pd.read_csv('/kaggle/input/mitsui-commodity-prediction-challenge/train_labels.csv')\n",
        "    target_pairs = pd.read_csv('/kaggle/input/mitsui-commodity-prediction-challenge/target_pairs.csv')\n",
        "    \n",
        "    print(f\"Train data shape: {train_data.shape}\")\n",
        "    print(f\"Test data shape: {test_data.shape}\")\n",
        "    print(f\"Train labels shape: {train_labels.shape}\")\n",
        "    print(f\"Target pairs shape: {target_pairs.shape}\")\n",
        "    \n",
        "    return train_data, test_data, train_labels, target_pairs\n",
        "\n",
        "def prepare_features(train_data, test_data):\n",
        "    \"\"\"Prepare features for deep learning models\"\"\"\n",
        "    print(\"Preparing features...\")\n",
        "    \n",
        "    # Find common columns\n",
        "    common_cols = list(set(train_data.columns) & set(test_data.columns))\n",
        "    print(f\"Common columns: {len(common_cols)}\")\n",
        "    \n",
        "    # Select numeric columns only\n",
        "    numeric_cols = []\n",
        "    for col in common_cols:\n",
        "        if col != 'date_id' and train_data[col].dtype in ['int64', 'float64']:\n",
        "            numeric_cols.append(col)\n",
        "    \n",
        "    print(f\"Numeric columns: {len(numeric_cols)}\")\n",
        "    \n",
        "    # Limit features for memory efficiency\n",
        "    if len(numeric_cols) > 500:\n",
        "        # Use variance-based selection\n",
        "        variances = train_data[numeric_cols].var()\n",
        "        top_features = variances.nlargest(500).index.tolist()\n",
        "        numeric_cols = top_features\n",
        "        print(f\"Selected top {len(numeric_cols)} features by variance\")\n",
        "    \n",
        "    # Prepare feature matrices\n",
        "    X_train = train_data[numeric_cols].fillna(0).values\n",
        "    X_test = test_data[numeric_cols].fillna(0).values\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    print(f\"Feature matrix shapes - Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n",
        "    \n",
        "    return X_train_scaled, X_test_scaled, numeric_cols, scaler\n",
        "\n",
        "def prepare_targets(train_labels):\n",
        "    \"\"\"Prepare target variables\"\"\"\n",
        "    print(\"Preparing targets...\")\n",
        "    \n",
        "    # Get target columns (excluding date_id)\n",
        "    target_cols = [col for col in train_labels.columns if col != 'date_id']\n",
        "    print(f\"Number of targets: {len(target_cols)}\")\n",
        "    \n",
        "    # Prepare target matrix\n",
        "    y_train = train_labels[target_cols].fillna(0).values\n",
        "    print(f\"Target matrix shape: {y_train.shape}\")\n",
        "    \n",
        "    return y_train, target_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deep Learning Model Definitions\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for time series data\"\"\"\n",
        "    def __init__(self, X, y, sequence_length=10):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.FloatTensor(y)\n",
        "        self.sequence_length = sequence_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X) - self.sequence_length\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            self.X[idx:idx + self.sequence_length],\n",
        "            self.y[idx + self.sequence_length]\n",
        "        )\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"LSTM model for time series prediction\"\"\"\n",
        "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        last_output = lstm_out[:, -1, :]\n",
        "        out = self.dropout(last_output)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "class GRUModel(nn.Module):\n",
        "    \"\"\"GRU model for time series prediction\"\"\"\n",
        "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        gru_out, _ = self.gru(x)\n",
        "        last_output = gru_out[:, -1, :]\n",
        "        out = self.dropout(last_output)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training functions\n",
        "def train_model(model, train_loader, val_loader, device, epochs=50, patience=10):\n",
        "    \"\"\"Train a deep learning model\"\"\"\n",
        "    model = model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y.unsqueeze(1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y.unsqueeze(1))\n",
        "                val_loss += loss.item()\n",
        "        \n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        \n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        \n",
        "        scheduler.step(avg_val_loss)\n",
        "        \n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}: Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
        "        \n",
        "        # Early stopping\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            \n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "    \n",
        "    return model, train_losses, val_losses\n",
        "\n",
        "def predict_with_model(model, test_loader, device):\n",
        "    \"\"\"Generate predictions with a trained model\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_X, _ in test_loader:\n",
        "            batch_X = batch_X.to(device)\n",
        "            outputs = model(batch_X)\n",
        "            predictions.extend(outputs.cpu().numpy().flatten())\n",
        "    \n",
        "    return np.array(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main training pipeline\n",
        "def train_deep_learning_models(X_train, y_train, target_cols, device):\n",
        "    \"\"\"Train multiple deep learning models for all targets\"\"\"\n",
        "    print(\"Training deep learning models...\")\n",
        "    \n",
        "    models = {}\n",
        "    predictions = {}\n",
        "    \n",
        "    # Train models for each target\n",
        "    for i, target_name in enumerate(target_cols):\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Processing target {i+1}/{len(target_cols)}: {target_name}\")\n",
        "        \n",
        "        # Prepare data for this target\n",
        "        y_target = y_train[:, i]\n",
        "        \n",
        "        # Split data\n",
        "        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "            X_train, y_target, test_size=0.2, random_state=42\n",
        "        )\n",
        "        \n",
        "        # Create datasets\n",
        "        sequence_length = 10\n",
        "        train_dataset = TimeSeriesDataset(X_train_split, y_train_split, sequence_length)\n",
        "        val_dataset = TimeSeriesDataset(X_val_split, y_val_split, sequence_length)\n",
        "        \n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "        \n",
        "        # Train LSTM model\n",
        "        lstm_model = LSTMModel(\n",
        "            input_size=X_train.shape[1],\n",
        "            hidden_size=64,  # Reduced for memory efficiency\n",
        "            num_layers=2,\n",
        "            dropout=0.2\n",
        "        )\n",
        "        \n",
        "        trained_lstm, _, _ = train_model(\n",
        "            lstm_model, train_loader, val_loader, device, epochs=30, patience=8\n",
        "        )\n",
        "        \n",
        "        # Store model and generate predictions\n",
        "        models[f\"lstm_{target_name}\"] = trained_lstm\n",
        "        \n",
        "        # Generate predictions for training data\n",
        "        train_dataset_full = TimeSeriesDataset(X_train, y_target, sequence_length)\n",
        "        train_loader_full = DataLoader(train_dataset_full, batch_size=32, shuffle=False)\n",
        "        \n",
        "        preds = predict_with_model(trained_lstm, train_loader_full, device)\n",
        "        predictions[f\"lstm_{target_name}\"] = preds\n",
        "        \n",
        "        # Clean up memory\n",
        "        del trained_lstm, train_dataset, val_dataset, train_loader, val_loader\n",
        "        gc.collect()\n",
        "        \n",
        "        # Limit to first 100 targets for memory efficiency\n",
        "        if i >= 99:\n",
        "            print(f\"Stopping at {i+1} targets for memory efficiency\")\n",
        "            break\n",
        "    \n",
        "    return models, predictions\n",
        "\n",
        "def generate_test_predictions(models, X_test, target_cols, device):\n",
        "    \"\"\"Generate predictions for test data\"\"\"\n",
        "    print(\"Generating test predictions...\")\n",
        "    \n",
        "    test_predictions = {}\n",
        "    sequence_length = 10\n",
        "    \n",
        "    # Process each target\n",
        "    for i, target_name in enumerate(target_cols):\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Generating predictions for target {i+1}/{len(target_cols)}: {target_name}\")\n",
        "        \n",
        "        model_key = f\"lstm_{target_name}\"\n",
        "        if model_key in models:\n",
        "            model = models[model_key]\n",
        "            \n",
        "            # Create test dataset\n",
        "            test_dataset = TimeSeriesDataset(X_test, np.zeros(len(X_test)), sequence_length)\n",
        "            test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "            \n",
        "            preds = predict_with_model(model, test_loader, device)\n",
        "            test_predictions[target_name] = preds\n",
        "        \n",
        "        # Limit to first 100 targets for memory efficiency\n",
        "        if i >= 99:\n",
        "            break\n",
        "    \n",
        "    return test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\ud83d\ude80 Starting MITSUI Deep Learning Competition Submission\")\n",
        "    \n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Load data\n",
        "    train_data, test_data, train_labels, target_pairs = load_data()\n",
        "    \n",
        "    # Prepare features and targets\n",
        "    X_train, X_test, feature_cols, scaler = prepare_features(train_data, test_data)\n",
        "    y_train, target_cols = prepare_targets(train_labels)\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcca Data Summary:\")\n",
        "    print(f\"Training samples: {X_train.shape[0]}\")\n",
        "    print(f\"Test samples: {X_test.shape[0]}\")\n",
        "    print(f\"Features: {X_train.shape[1]}\")\n",
        "    print(f\"Targets: {len(target_cols)}\")\n",
        "    \n",
        "    # Train models\n",
        "    models, train_predictions = train_deep_learning_models(X_train, y_train, target_cols, device)\n",
        "    \n",
        "    # Generate test predictions\n",
        "    test_predictions = generate_test_predictions(models, X_test, target_cols, device)\n",
        "    \n",
        "    print(\"\\n\u2705 Training and prediction completed successfully!\")\n",
        "    print(f\"Trained models: {len(models)}\")\n",
        "    print(f\"Generated predictions for {len(test_predictions)} targets\")\n",
        "    \n",
        "    # Save results\n",
        "    print(\"\\n\ud83d\udcbe Saving results...\")\n",
        "    \n",
        "    # Create submission dataframe\n",
        "    submission_data = []\n",
        "    for target_name in test_predictions.keys():\n",
        "        preds = test_predictions[target_name]\n",
        "        for i, pred in enumerate(preds):\n",
        "            submission_data.append({\n",
        "                'date_id': test_data['date_id'].iloc[i + 10],  # +10 for sequence length offset\n",
        "                'target': target_name,\n",
        "                'value': pred\n",
        "            })\n",
        "    \n",
        "    submission_df = pd.DataFrame(submission_data)\n",
        "    submission_df.to_csv('submission.csv', index=False)\n",
        "    \n",
        "    print(f\"\\n\ud83c\udfaf Submission file created: submission.csv\")\n",
        "    print(f\"Submission shape: {submission_df.shape}\")\n",
        "    print(f\"\\n\ud83c\udfc6 Ready for Kaggle submission!\")\n",
        "    \n",
        "    # Display sample predictions\n",
        "    print(\"\\n\ud83d\udccb Sample predictions:\")\n",
        "    print(submission_df.head(10))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}